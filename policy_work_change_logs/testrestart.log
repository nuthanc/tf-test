2021-02-19 05:41:16,356 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 05:41:17,478 - DEBUG - Output : noden20
2021-02-19 05:41:17,478 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 05:41:17,593 - DEBUG - Output : noden20.maas
2021-02-19 05:41:17,593 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 05:41:17,823 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 05:41:17,824 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 05:41:17,933 - DEBUG - Output : noden20.maas
2021-02-19 05:41:17,934 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 05:41:18,054 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 05:41:18,055 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 05:41:19,075 - DEBUG - Output : noden29
2021-02-19 05:41:19,075 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 05:41:19,179 - DEBUG - Output : noden29.maas
2021-02-19 05:41:19,179 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 05:41:19,390 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 05:41:19,391 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 05:41:19,569 - DEBUG - Output : 
2021-02-19 05:41:19,570 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 05:41:19,679 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 05:41:19,870 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 05:41:21,158 - DEBUG - Output : nodei34
2021-02-19 05:41:21,159 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 05:41:21,266 - DEBUG - Output : nodei34.maas
2021-02-19 05:41:21,267 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 05:41:21,498 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 05:41:21,500 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 05:41:21,608 - DEBUG - Output : nodei34.maas
2021-02-19 05:41:21,609 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 05:41:21,726 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 05:41:21,727 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 05:41:22,758 - DEBUG - Output : noden19
2021-02-19 05:41:22,759 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 05:41:22,867 - DEBUG - Output : noden19.maas
2021-02-19 05:41:22,868 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 05:41:23,027 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 05:41:23,028 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 05:41:23,133 - DEBUG - Output : 192.168.27.19/24
2021-02-19 05:41:23,133 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 05:41:23,237 - DEBUG - Output : noden19.maas
2021-02-19 05:41:23,237 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 05:41:23,932 - DEBUG - Output : nodec9
2021-02-19 05:41:23,933 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 05:41:24,015 - DEBUG - Output : nodec9.maas
2021-02-19 05:41:24,015 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 05:41:24,138 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 05:41:24,139 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 05:41:24,219 - DEBUG - Output : 192.168.27.9/24
2021-02-19 05:41:24,219 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 05:41:24,304 - DEBUG - Output : nodec9.maas
2021-02-19 05:41:24,304 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 05:41:24,676 - DEBUG - Output : noden18
2021-02-19 05:41:24,677 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 05:41:24,743 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 05:41:24,744 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 05:41:24,917 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 05:41:26,835 - DEBUG - Not creating keypair since it exists
2021-02-19 05:41:27,571 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 05:41:27,706 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 05:41:27,739 - INFO - Adding rules to the default security group in Project admin
2021-02-19 05:41:27,924 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:41:28,256 - INFO - ================================================================================
2021-02-19 05:41:28,257 - INFO - STARTING TEST    : test_admin_project_with_agent_restart
2021-02-19 05:41:28,258 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after vrouter agent restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart vrouter agent
                5. Perform create and delete operations again
         Pass criteria: Even after vrouter agent restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 05:41:29,572 - DEBUG - Skipping xmpp flap check
2021-02-19 05:41:29,572 - INFO - Initial checks done. Running the testcase now
2021-02-19 05:41:29,572 - INFO - 
2021-02-19 05:41:30,985 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:42:10,257 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:42:46,960 - INFO - Will restart contrail-vrouter-agent  services now on ['192.168.7.19', '192.168.7.9']
2021-02-19 05:42:46,961 - INFO - Running docker restart vrouter_vrouter-agent_1 -t 60 on noden19
2021-02-19 05:42:46,962 - DEBUG - [192.168.7.19]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 05:42:51,470 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 05:42:51,470 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:42:51,643 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 05:42:51,644 - INFO - 192.168.7.19
2021-02-19 05:42:51,659 - INFO -     agent:initializing
2021-02-19 05:42:51,660 - DEBUG - defaultdict(<class 'dict'>, {'192.168.7.19': {'agent': {'status': 'initializing', 'description': None}}})
2021-02-19 05:42:51,660 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 05:42:56,661 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:42:56,797 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 05:42:56,798 - INFO - 192.168.7.19
2021-02-19 05:42:56,831 - INFO -     agent:active
2021-02-19 05:42:56,831 - INFO - Contrail services ['agent'] are up on nodes 192.168.7.19
2021-02-19 05:42:56,831 - INFO - Running docker restart vrouter_vrouter-agent_1 -t 60 on nodec9
2021-02-19 05:42:56,832 - DEBUG - [192.168.7.9]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 05:43:01,324 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 05:43:01,324 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:43:01,425 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 05:43:01,425 - INFO - 192.168.7.9
2021-02-19 05:43:01,434 - INFO -     agent:initializing
2021-02-19 05:43:01,435 - DEBUG - defaultdict(<class 'dict'>, {'192.168.7.9': {'agent': {'status': 'initializing', 'description': None}}})
2021-02-19 05:43:01,435 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 05:43:06,436 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:43:06,528 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 05:43:06,528 - INFO - 192.168.7.9
2021-02-19 05:43:06,553 - INFO -     agent:active
2021-02-19 05:43:06,553 - INFO - Contrail services ['agent'] are up on nodes 192.168.7.9
2021-02-19 05:43:06,557 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:43:15,428 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:43:51,856 - DEBUG - Skipping xmpp flap check
2021-02-19 05:43:51,857 - INFO - END TEST : test_admin_project_with_agent_restart : PASSED[0:02:23]
2021-02-19 05:43:51,857 - INFO - --------------------------------------------------------------------------------
2021-02-19 05:43:51,864 - INFO - ================================================================================
2021-02-19 05:43:51,864 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 05:43:51,865 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 05:43:53,164 - DEBUG - Skipping xmpp flap check
2021-02-19 05:43:53,165 - INFO - Initial checks done. Running the testcase now
2021-02-19 05:43:53,165 - INFO - 
2021-02-19 05:43:54,639 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:44:11,584 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:44:48,259 - INFO - Will restart contrail-kube-manager  services now on ['192.168.7.29']
2021-02-19 05:44:48,259 - INFO - Unable to find contrail-kube-manager container on 192.168.7.29
2021-02-19 05:45:18,268 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:45:30,107 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:46:12,888 - DEBUG - Skipping xmpp flap check
2021-02-19 05:46:12,889 - INFO - END TEST : test_admin_project_with_kube_manager_restart : PASSED[0:02:21]
2021-02-19 05:46:12,889 - INFO - --------------------------------------------------------------------------------
2021-02-19 05:46:12,896 - INFO - ================================================================================
2021-02-19 05:46:12,896 - INFO - STARTING TEST    : test_custom_project_with_agent_restart
2021-02-19 05:46:12,896 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after vrouter agent restart for custom user
         Test steps:
                1. Create stackrc_dict for custom user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart vrouter agent
                5. Perform create and delete operations again
         Pass criteria: Even after vrouter agent restart, custom user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 05:46:14,224 - DEBUG - Skipping xmpp flap check
2021-02-19 05:46:14,224 - INFO - Initial checks done. Running the testcase now
2021-02-19 05:46:14,224 - INFO - 
2021-02-19 05:46:16,498 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:46:55,533 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:47:28,547 - INFO - Will restart contrail-vrouter-agent  services now on ['192.168.7.19', '192.168.7.9']
2021-02-19 05:47:28,547 - INFO - Running docker restart vrouter_vrouter-agent_1 -t 60 on noden19
2021-02-19 05:47:28,548 - DEBUG - [192.168.7.19]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 05:47:33,033 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 05:47:33,034 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:47:33,207 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 05:47:33,208 - INFO - 192.168.7.19
2021-02-19 05:47:33,219 - INFO -     agent:initializing
2021-02-19 05:47:33,220 - DEBUG - defaultdict(<class 'dict'>, {'192.168.7.19': {'agent': {'status': 'initializing', 'description': None}}})
2021-02-19 05:47:33,220 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 05:47:38,220 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:47:38,360 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 05:47:38,361 - INFO - 192.168.7.19
2021-02-19 05:47:38,391 - INFO -     agent:active
2021-02-19 05:47:38,392 - INFO - Contrail services ['agent'] are up on nodes 192.168.7.19
2021-02-19 05:47:38,392 - INFO - Running docker restart vrouter_vrouter-agent_1 -t 60 on nodec9
2021-02-19 05:47:38,392 - DEBUG - [192.168.7.9]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 05:47:42,771 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 05:47:42,773 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:47:42,879 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 05:47:42,879 - INFO - 192.168.7.9
2021-02-19 05:47:42,890 - INFO -     agent:initializing
2021-02-19 05:47:42,891 - DEBUG - defaultdict(<class 'dict'>, {'192.168.7.9': {'agent': {'status': 'initializing', 'description': None}}})
2021-02-19 05:47:42,891 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 05:47:47,893 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:47:47,988 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 05:47:47,988 - INFO - 192.168.7.9
2021-02-19 05:47:48,016 - INFO -     agent:active
2021-02-19 05:47:48,017 - INFO - Contrail services ['agent'] are up on nodes 192.168.7.9
2021-02-19 05:47:48,020 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:48:00,873 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:48:35,936 - DEBUG - Skipping xmpp flap check
2021-02-19 05:48:35,937 - INFO - END TEST : test_custom_project_with_agent_restart : PASSED[0:02:23]
2021-02-19 05:48:35,937 - INFO - --------------------------------------------------------------------------------
2021-02-19 05:48:35,943 - INFO - ================================================================================
2021-02-19 05:48:35,944 - INFO - STARTING TEST    : test_custom_project_with_kube_manager_restart
2021-02-19 05:48:35,944 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart for custom user
         Test steps:
                1. Create stackrc_dict for custom user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, custom user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 05:48:37,268 - DEBUG - Skipping xmpp flap check
2021-02-19 05:48:37,269 - INFO - Initial checks done. Running the testcase now
2021-02-19 05:48:37,269 - INFO - 
2021-02-19 05:48:40,587 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:48:50,432 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:49:26,337 - INFO - Will restart contrail-kube-manager  services now on ['192.168.7.29']
2021-02-19 05:49:26,337 - INFO - Unable to find contrail-kube-manager container on 192.168.7.29
2021-02-19 05:49:56,343 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:50:11,275 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 05:50:52,620 - DEBUG - Skipping xmpp flap check
2021-02-19 05:50:52,621 - INFO - END TEST : test_custom_project_with_kube_manager_restart : PASSED[0:02:17]
2021-02-19 05:50:52,621 - INFO - --------------------------------------------------------------------------------
2021-02-19 05:52:16,428 - DEBUG - [192.168.7.19]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 05:52:20,966 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 05:52:20,967 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:52:21,126 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 05:52:26,140 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:52:26,280 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 05:52:26,313 - DEBUG - [192.168.7.9]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 05:52:30,686 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 05:52:30,687 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:52:30,799 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 05:52:35,811 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 05:52:35,894 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 06:12:00,963 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 06:12:02,101 - DEBUG - Output : noden20
2021-02-19 06:12:02,101 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 06:12:02,218 - DEBUG - Output : noden20.maas
2021-02-19 06:12:02,218 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:12:02,440 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 06:12:02,440 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 06:12:02,569 - DEBUG - Output : noden20.maas
2021-02-19 06:12:02,569 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:12:02,690 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 06:12:02,690 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 06:12:03,747 - DEBUG - Output : noden29
2021-02-19 06:12:03,748 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 06:12:03,849 - DEBUG - Output : noden29.maas
2021-02-19 06:12:03,849 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:12:04,069 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 06:12:04,069 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 06:12:04,191 - DEBUG - Output : 
2021-02-19 06:12:04,192 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:12:04,293 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 06:12:04,484 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 06:12:05,739 - DEBUG - Output : nodei34
2021-02-19 06:12:05,739 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 06:12:05,847 - DEBUG - Output : nodei34.maas
2021-02-19 06:12:05,848 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:12:06,080 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 06:12:06,082 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 06:12:06,192 - DEBUG - Output : nodei34.maas
2021-02-19 06:12:06,192 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:12:06,294 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 06:12:06,295 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 06:12:07,344 - DEBUG - Output : noden19
2021-02-19 06:12:07,344 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 06:12:07,442 - DEBUG - Output : noden19.maas
2021-02-19 06:12:07,442 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:12:07,609 - DEBUG - Output : NAMES
pedantic_morse
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 06:12:07,610 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:12:07,717 - DEBUG - Output : 192.168.27.19/24
2021-02-19 06:12:07,718 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 06:12:07,821 - DEBUG - Output : noden19.maas
2021-02-19 06:12:07,822 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 06:12:08,553 - DEBUG - Output : nodec9
2021-02-19 06:12:08,554 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 06:12:08,631 - DEBUG - Output : nodec9.maas
2021-02-19 06:12:08,631 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:12:08,756 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 06:12:08,757 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:12:08,839 - DEBUG - Output : 192.168.27.9/24
2021-02-19 06:12:08,840 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 06:12:08,922 - DEBUG - Output : nodec9.maas
2021-02-19 06:12:08,923 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 06:12:09,316 - DEBUG - Output : noden18
2021-02-19 06:12:09,316 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 06:12:09,372 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 06:12:09,372 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:12:09,566 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 06:12:11,483 - DEBUG - Not creating keypair since it exists
2021-02-19 06:12:12,753 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:12:12,881 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:12:12,909 - INFO - Adding rules to the default security group in Project admin
2021-02-19 06:12:13,058 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:12:13,415 - INFO - ================================================================================
2021-02-19 06:12:13,416 - INFO - STARTING TEST    : test_admin_project_with_agent_restart
2021-02-19 06:12:13,417 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after vrouter agent restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart vrouter agent
                5. Perform create and delete operations again
         Pass criteria: Even after vrouter agent restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:12:14,748 - DEBUG - Skipping xmpp flap check
2021-02-19 06:12:14,748 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:12:14,749 - INFO - 
2021-02-19 06:12:16,199 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:12:46,722 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:13:26,745 - INFO - Will restart contrail-vrouter-agent  services now on ['192.168.7.19', '192.168.7.9']
2021-02-19 06:13:26,746 - INFO - Running docker restart vrouter_vrouter-agent_1 -t 60 on noden19
2021-02-19 06:13:26,746 - DEBUG - [192.168.7.19]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 06:13:31,257 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 06:13:31,258 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 06:13:31,426 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 06:13:31,426 - INFO - 192.168.7.19
2021-02-19 06:13:31,439 - INFO -     agent:initializing
2021-02-19 06:13:31,440 - DEBUG - defaultdict(<class 'dict'>, {'192.168.7.19': {'agent': {'status': 'initializing', 'description': None}}})
2021-02-19 06:13:31,440 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 06:13:36,442 - DEBUG - [192.168.7.19]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 06:13:36,579 - DEBUG - Output : vrouter_provisioner_1
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_POD_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_POD_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
k8s_POD_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_49
k8s_POD_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_31
k8s_POD_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_31
2021-02-19 06:13:36,579 - INFO - 192.168.7.19
2021-02-19 06:13:36,614 - INFO -     agent:active
2021-02-19 06:13:36,615 - INFO - Contrail services ['agent'] are up on nodes 192.168.7.19
2021-02-19 06:13:36,615 - INFO - Running docker restart vrouter_vrouter-agent_1 -t 60 on nodec9
2021-02-19 06:13:36,615 - DEBUG - [192.168.7.9]: Running cmd : docker restart vrouter_vrouter-agent_1 -t 60
2021-02-19 06:13:41,124 - DEBUG - Output : vrouter_vrouter-agent_1
2021-02-19 06:13:41,125 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 06:13:41,226 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 06:13:41,226 - INFO - 192.168.7.9
2021-02-19 06:13:41,236 - INFO -     agent:initializing
2021-02-19 06:13:41,236 - DEBUG - defaultdict(<class 'dict'>, {'192.168.7.9': {'agent': {'status': 'initializing', 'description': None}}})
2021-02-19 06:13:41,237 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 06:13:46,238 - DEBUG - [192.168.7.9]: Running cmd : docker ps -f status=running --format {{.Names}} 2>/dev/null
2021-02-19 06:13:46,326 - DEBUG - Output : vrouter_vrouter-agent_1
vrouter_provisioner_1
vrouter_nodemgr_1
2021-02-19 06:13:46,327 - INFO - 192.168.7.9
2021-02-19 06:13:46,352 - INFO -     agent:active
2021-02-19 06:13:46,353 - INFO - Contrail services ['agent'] are up on nodes 192.168.7.9
2021-02-19 06:13:46,357 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:13:54,186 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:14:35,872 - DEBUG - Skipping xmpp flap check
2021-02-19 06:14:35,872 - INFO - END TEST : test_admin_project_with_agent_restart : PASSED[0:02:22]
2021-02-19 06:14:35,873 - INFO - --------------------------------------------------------------------------------
2021-02-19 06:14:35,880 - INFO - ================================================================================
2021-02-19 06:14:35,880 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 06:14:35,880 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:14:37,220 - DEBUG - Skipping xmpp flap check
2021-02-19 06:14:37,220 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:14:37,220 - INFO - 
2021-02-19 06:14:38,601 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:14:55,476 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:17:36,893 - INFO - Will restart contrail-kube-manager  services now on ['192.168.7.29']
2021-02-19 06:24:32,480 - ERROR - BdbQuit
Python 3.6.8: /usr/bin/python3
Fri Feb 19 06:24:31 2021

A problem occurred in a Python script.  Here is the sequence of
function calls leading up to the error, in the order they occurred.

 /contrail-test/tcutils/wrappers.py in wrapper(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f33728d3240>, *args=(), **kwargs={})
   80             log.info('Initial checks done. Running the testcase now')
   81             log.info('')
   82             result = function(self, *args, **kwargs)
   83             if self.inputs.upgrade:
   84                 pid = os.getpid()
result = None
function = <function TestRestart.test_admin_project_with_kube_manager_restart>
self = <serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f33728d3240>
args = ()
kwargs = {}

 /contrail-test/serial_scripts/k8s_auth/test_restart.py in test_admin_project_with_kube_manager_restart(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f33728d3240>)
   42             stackrc_dict=stackrc_dict, inputs=self.inputs)
   43         import pdb;pdb.set_trace()
   44         self.restart_kube_manager()
   45         self.inputs.restart_service('contrail-kube-manager', ips,
   46                                      container='contrailkubernetesmaster_kubemanager_1',
self = <serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f33728d3240>
self.restart_kube_manager = <bound method BaseK8sTest.restart_kube_manager o...th_kube_manager_restart[auth] id=0x7f33728d3240>>

 /contrail-test/common/k8s/base.py in restart_kube_manager(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f33728d3240>, ips=['192.168.7.29'])
  988         self.inputs.restart_service('contrail-kube-manager', ips,
  989                                      container='contrail-kube-manager',
  990                                      verify_service=True)
  991         time.sleep(30)#wait time to stabilize the cluster
  992     # end restart_kube_manager
verify_service undefined

 /contrail-test/common/contrail_test_init.py in restart_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', host_ips=['192.168.7.29'], container='contrail-kube-manager', verify_service=True)
 1437                         container=None, verify_service=True):
 1438         self._action_on_service(service_name, 'restart', host_ips, container,
 1439             verify_service=verify_service)
 1440     # end restart_service
 1441 
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', event='restart', host_ips=['192.168.7.29'], container='contrail-kube-manager', verify_service=True)
 1417         if self.is_microservices_env and container:
 1418             return self._action_on_container(host_ips, event, container, services=services,
 1419                                              verify_service=verify_service)
 1420         _container = container
 1421         for service in services:
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_container(self=<common.contrail_test_init.ContrailTestInit object>, hosts=['192.168.7.29'], event='restart', container='contrail-kube-manager', services=['contrail-kube-manager'], verify_service=True, timeout=60)
 1363             host_ip = self.host_data[host]['host_ip']
 1364             for container in containers:
 1365                 cntr = self.get_container_name(host, container)
 1366                 if not cntr:
 1367                     self.logger.info('Unable to find %s container on %s'%(container, host))
cntr = None
self = <common.contrail_test_init.ContrailTestInit object>
self.get_container_name = <bound method ContrailTestInit.get_container_nam...mmon.contrail_test_init.ContrailTestInit object>>
host = '192.168.7.29'
container = 'contrail-kube-manager'

 /contrail-test/common/contrail_test_init.py in get_container_name(self=<common.contrail_test_init.ContrailTestInit object>, host='192.168.7.29', service='contrail-kube-manager')
 1317            service - contrail service (eg: agent)
 1318         '''
 1319         return self.host_data[host].get('containers', {}).get(service)
 1320 
 1321     def is_container_up(self, host, service):
self = <common.contrail_test_init.ContrailTestInit object>
self.host_data = {'192.168.27.19': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0': 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0', 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0': 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0', 'pedantic_morse': 'pedantic_morse', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.19', 'control_data_ip': '192.168.27.19', 'data-ip': '192.168.27.19', 'fqname': 'noden19.maas', 'host_control_ip': '192.168.27.19', 'host_data_ip': '192.168.27.19', 'host_ip': '192.168.7.19', 'name': 'noden19', 'password': 'c0ntrail123', ...}, '192.168.27.9': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, '192.168.7.18': {'containers': {'NAMES': 'NAMES', 'nuthan_test': 'nuthan_test', 'nuthan_test2': 'nuthan_test2', 'nuthan_test7': 'nuthan_test7', 'nuthan_test8': 'nuthan_test8', 'scale_test': 'scale_test', 'tf_test': 'tf_test'}, 'control-ip': '192.168.7.18', 'data-ip': '192.168.7.18', 'fqname': 'noden18.englab.juniper.net', 'host_control_ip': '192.168.7.18', 'host_data_ip': '192.168.7.18', 'host_ip': '192.168.7.18', 'name': 'noden18', 'password': 'c0ntrail123', 'roles': {}, ...}, '192.168.7.19': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0': 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0', 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0': 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0', 'pedantic_morse': 'pedantic_morse', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.19', 'control_data_ip': '192.168.27.19', 'data-ip': '192.168.27.19', 'fqname': 'noden19.maas', 'host_control_ip': '192.168.27.19', 'host_data_ip': '192.168.27.19', 'host_ip': '192.168.7.19', 'name': 'noden19', 'password': 'c0ntrail123', ...}, '192.168.7.20': {'containers': {'NAMES': 'NAMES', 'alarmgen': 'analyticsalarm_alarm-gen_1', 'analytics-api': 'analytics_api_1', 'analytics-cassandra': 'analyticsdatabase_cassandra_1', 'analytics-nodemgr': 'analytics_nodemgr_1', 'analyticsalarm_kafka_1': 'analyticsalarm_kafka_1', 'analyticsdb-nodemgr': 'analyticsdatabase_nodemgr_1', 'api-server': 'configapi_api_1', 'collector': 'analytics_collector_1', 'config-cassandra': 'configdatabase_cassandra_1', ...}, 'control-ip': '192.168.7.20', 'data-ip': '192.168.7.20', 'fqname': 'noden20.maas', 'host_control_ip': '192.168.7.20', 'host_data_ip': '192.168.7.20', 'host_ip': '192.168.7.20', 'ips': ['192.168.27.20', '172.17.0.1', '192.168.7.20'], 'name': 'noden20', 'password': 'c0ntrail123', ...}, '192.168.7.29': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'alarmgen': 'analyticsalarm_alarm-gen_1', 'analytics-api': 'analytics_api_1', 'analytics-cassandra': 'analyticsdatabase_cassandra_1', 'analytics-nodemgr': 'analytics_nodemgr_1', 'analyticsalarm_kafka_1': 'analyticsalarm_kafka_1', 'analyticsdb-nodemgr': 'analyticsdatabase_nodemgr_1', 'api-server': 'configapi_api_1', 'collector': 'analytics_collector_1', ...}, 'control-ip': '192.168.7.29', 'data-ip': '192.168.7.29', 'fqname': 'noden29.maas', 'host_control_ip': '192.168.7.29', 'host_data_ip': '192.168.7.29', 'host_ip': '192.168.7.29', 'ips': ['192.168.7.29', '172.17.0.1', '192.168.27.29'], 'name': 'noden29', 'password': 'c0ntrail123', ...}, '192.168.7.34': {'containers': {'NAMES': 'NAMES', 'alarmgen': 'analyticsalarm_alarm-gen_1', 'analytics-api': 'analytics_api_1', 'analytics-cassandra': 'analyticsdatabase_cassandra_1', 'analytics-nodemgr': 'analytics_nodemgr_1', 'analyticsalarm_kafka_1': 'analyticsalarm_kafka_1', 'analyticsdb-nodemgr': 'analyticsdatabase_nodemgr_1', 'api-server': 'configapi_api_1', 'collector': 'analytics_collector_1', 'config-cassandra': 'configdatabase_cassandra_1', ...}, 'control-ip': '192.168.7.34', 'data-ip': '192.168.7.34', 'fqname': 'nodei34.maas', 'host_control_ip': '192.168.7.34', 'host_data_ip': '192.168.7.34', 'host_ip': '192.168.7.34', 'ips': ['192.168.7.34', '192.168.27.34', '172.17.0.1'], 'name': 'nodei34', 'password': 'c0ntrail123', ...}, '192.168.7.9': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, 'nodec9': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, 'nodec9.maas': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, ...}
host = '192.168.7.29'
].get undefined
service = 'contrail-kube-manager'

 /contrail-test/common/contrail_test_init.py in get_container_name(self=<common.contrail_test_init.ContrailTestInit object>, host='192.168.7.29', service='contrail-kube-manager')
 1317            service - contrail service (eg: agent)
 1318         '''
 1319         return self.host_data[host].get('containers', {}).get(service)
 1320 
 1321     def is_container_up(self, host, service):
self = <common.contrail_test_init.ContrailTestInit object>
self.host_data = {'192.168.27.19': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0': 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0', 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0': 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0', 'pedantic_morse': 'pedantic_morse', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.19', 'control_data_ip': '192.168.27.19', 'data-ip': '192.168.27.19', 'fqname': 'noden19.maas', 'host_control_ip': '192.168.27.19', 'host_data_ip': '192.168.27.19', 'host_ip': '192.168.7.19', 'name': 'noden19', 'password': 'c0ntrail123', ...}, '192.168.27.9': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, '192.168.7.18': {'containers': {'NAMES': 'NAMES', 'nuthan_test': 'nuthan_test', 'nuthan_test2': 'nuthan_test2', 'nuthan_test7': 'nuthan_test7', 'nuthan_test8': 'nuthan_test8', 'scale_test': 'scale_test', 'tf_test': 'tf_test'}, 'control-ip': '192.168.7.18', 'data-ip': '192.168.7.18', 'fqname': 'noden18.englab.juniper.net', 'host_control_ip': '192.168.7.18', 'host_data_ip': '192.168.7.18', 'host_ip': '192.168.7.18', 'name': 'noden18', 'password': 'c0ntrail123', 'roles': {}, ...}, '192.168.7.19': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0': 'k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22', 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23': 'k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0', 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0': 'k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d69...ube-system_64575096-0773-4881-8bba-f16559ca5fd6_0', 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0': 'k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0', 'pedantic_morse': 'pedantic_morse', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.19', 'control_data_ip': '192.168.27.19', 'data-ip': '192.168.27.19', 'fqname': 'noden19.maas', 'host_control_ip': '192.168.27.19', 'host_data_ip': '192.168.27.19', 'host_ip': '192.168.7.19', 'name': 'noden19', 'password': 'c0ntrail123', ...}, '192.168.7.20': {'containers': {'NAMES': 'NAMES', 'alarmgen': 'analyticsalarm_alarm-gen_1', 'analytics-api': 'analytics_api_1', 'analytics-cassandra': 'analyticsdatabase_cassandra_1', 'analytics-nodemgr': 'analytics_nodemgr_1', 'analyticsalarm_kafka_1': 'analyticsalarm_kafka_1', 'analyticsdb-nodemgr': 'analyticsdatabase_nodemgr_1', 'api-server': 'configapi_api_1', 'collector': 'analytics_collector_1', 'config-cassandra': 'configdatabase_cassandra_1', ...}, 'control-ip': '192.168.7.20', 'data-ip': '192.168.7.20', 'fqname': 'noden20.maas', 'host_control_ip': '192.168.7.20', 'host_data_ip': '192.168.7.20', 'host_ip': '192.168.7.20', 'ips': ['192.168.27.20', '172.17.0.1', '192.168.7.20'], 'name': 'noden20', 'password': 'c0ntrail123', ...}, '192.168.7.29': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'alarmgen': 'analyticsalarm_alarm-gen_1', 'analytics-api': 'analytics_api_1', 'analytics-cassandra': 'analyticsdatabase_cassandra_1', 'analytics-nodemgr': 'analytics_nodemgr_1', 'analyticsalarm_kafka_1': 'analyticsalarm_kafka_1', 'analyticsdb-nodemgr': 'analyticsdatabase_nodemgr_1', 'api-server': 'configapi_api_1', 'collector': 'analytics_collector_1', ...}, 'control-ip': '192.168.7.29', 'data-ip': '192.168.7.29', 'fqname': 'noden29.maas', 'host_control_ip': '192.168.7.29', 'host_data_ip': '192.168.7.29', 'host_ip': '192.168.7.29', 'ips': ['192.168.7.29', '172.17.0.1', '192.168.27.29'], 'name': 'noden29', 'password': 'c0ntrail123', ...}, '192.168.7.34': {'containers': {'NAMES': 'NAMES', 'alarmgen': 'analyticsalarm_alarm-gen_1', 'analytics-api': 'analytics_api_1', 'analytics-cassandra': 'analyticsdatabase_cassandra_1', 'analytics-nodemgr': 'analytics_nodemgr_1', 'analyticsalarm_kafka_1': 'analyticsalarm_kafka_1', 'analyticsdb-nodemgr': 'analyticsdatabase_nodemgr_1', 'api-server': 'configapi_api_1', 'collector': 'analytics_collector_1', 'config-cassandra': 'configdatabase_cassandra_1', ...}, 'control-ip': '192.168.7.34', 'data-ip': '192.168.7.34', 'fqname': 'nodei34.maas', 'host_control_ip': '192.168.7.34', 'host_data_ip': '192.168.7.34', 'host_ip': '192.168.7.34', 'ips': ['192.168.7.34', '192.168.27.34', '172.17.0.1'], 'name': 'nodei34', 'password': 'c0ntrail123', ...}, '192.168.7.9': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, 'nodec9': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, 'nodec9.maas': {'containers': {'NAMES': 'NAMES', 'agent': 'vrouter_vrouter-agent_1', 'vrouter-nodemgr': 'vrouter_nodemgr_1'}, 'control-ip': '192.168.27.9', 'control_data_ip': '192.168.27.9', 'data-ip': '192.168.27.9', 'fqname': 'nodec9.maas', 'host_control_ip': '192.168.27.9', 'host_data_ip': '192.168.27.9', 'host_ip': '192.168.7.9', 'name': 'nodec9', 'password': 'c0ntrail123', ...}, ...}
host = '192.168.7.29'
].get undefined
service = 'contrail-kube-manager'

 /usr/lib64/python3.6/bdb.py in trace_dispatch(self=<pdb.Pdb object>, frame=<frame object>, event='line', arg=None)
   49             return # None
   50         if event == 'line':
   51             return self.dispatch_line(frame)
   52         if event == 'call':
   53             return self.dispatch_call(frame, arg)
self = <pdb.Pdb object>
self.dispatch_line = <bound method Bdb.dispatch_line of <pdb.Pdb object>>
frame = <frame object>

 /usr/lib64/python3.6/bdb.py in dispatch_line(self=<pdb.Pdb object>, frame=<frame object>)
   68         if self.stop_here(frame) or self.break_here(frame):
   69             self.user_line(frame)
   70             if self.quitting: raise BdbQuit
   71         return self.trace_dispatch
   72 
self = <pdb.Pdb object>
self.quitting = True
global BdbQuit = <class 'bdb.BdbQuit'>
BdbQuit: 
    __cause__ = None
    __class__ = <class 'bdb.BdbQuit'>
    __context__ = None
    __delattr__ = <method-wrapper '__delattr__' of BdbQuit object>
    __dict__ = {}
    __dir__ = <built-in method __dir__ of BdbQuit object>
    __doc__ = 'Exception to give up completely.'
    __eq__ = <method-wrapper '__eq__' of BdbQuit object>
    __format__ = <built-in method __format__ of BdbQuit object>
    __ge__ = <method-wrapper '__ge__' of BdbQuit object>
    __getattribute__ = <method-wrapper '__getattribute__' of BdbQuit object>
    __gt__ = <method-wrapper '__gt__' of BdbQuit object>
    __hash__ = <method-wrapper '__hash__' of BdbQuit object>
    __init__ = <method-wrapper '__init__' of BdbQuit object>
    __init_subclass__ = <built-in method __init_subclass__ of type object>
    __le__ = <method-wrapper '__le__' of BdbQuit object>
    __lt__ = <method-wrapper '__lt__' of BdbQuit object>
    __module__ = 'bdb'
    __ne__ = <method-wrapper '__ne__' of BdbQuit object>
    __new__ = <built-in method __new__ of type object>
    __reduce__ = <built-in method __reduce__ of BdbQuit object>
    __reduce_ex__ = <built-in method __reduce_ex__ of BdbQuit object>
    __repr__ = <method-wrapper '__repr__' of BdbQuit object>
    __setattr__ = <method-wrapper '__setattr__' of BdbQuit object>
    __setstate__ = <built-in method __setstate__ of BdbQuit object>
    __sizeof__ = <built-in method __sizeof__ of BdbQuit object>
    __str__ = <method-wrapper '__str__' of BdbQuit object>
    __subclasshook__ = <built-in method __subclasshook__ of type object>
    __suppress_context__ = False
    __traceback__ = <traceback object>
    __weakref__ = None
    args = ()
    with_traceback = <built-in method with_traceback of BdbQuit object>

The above is a description of an error in a Python program.  Here is
the original traceback:

Traceback (most recent call last):
  File "/contrail-test/tcutils/wrappers.py", line 82, in wrapper
    result = function(self, *args, **kwargs)
  File "/contrail-test/serial_scripts/k8s_auth/test_restart.py", line 44, in test_admin_project_with_kube_manager_restart
    self.restart_kube_manager()
  File "/contrail-test/common/k8s/base.py", line 990, in restart_kube_manager
    verify_service=True)
  File "/contrail-test/common/contrail_test_init.py", line 1439, in restart_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1419, in _action_on_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1365, in _action_on_container
    cntr = self.get_container_name(host, container)
  File "/contrail-test/common/contrail_test_init.py", line 1319, in get_container_name
    return self.host_data[host].get('containers', {}).get(service)
  File "/contrail-test/common/contrail_test_init.py", line 1319, in get_container_name
    return self.host_data[host].get('containers', {}).get(service)
  File "/usr/lib64/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/lib64/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit



2021-02-19 06:24:32,481 - DEBUG - Skipping xmpp flap check
2021-02-19 06:24:32,481 - INFO - 
2021-02-19 06:24:32,481 - INFO - END TEST : test_admin_project_with_kube_manager_restart : FAILED[0:09:57]
2021-02-19 06:24:32,481 - INFO - --------------------------------------------------------------------------------
2021-02-19 06:24:32,488 - INFO - ================================================================================
2021-02-19 06:24:32,488 - INFO - STARTING TEST    : test_custom_project_with_agent_restart
2021-02-19 06:24:32,489 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after vrouter agent restart for custom user
         Test steps:
                1. Create stackrc_dict for custom user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart vrouter agent
                5. Perform create and delete operations again
         Pass criteria: Even after vrouter agent restart, custom user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:24:33,820 - DEBUG - Skipping xmpp flap check
2021-02-19 06:24:33,821 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:24:33,821 - INFO - 
2021-02-19 06:26:21,017 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 06:26:22,314 - DEBUG - Output : noden20
2021-02-19 06:26:22,315 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 06:26:22,433 - DEBUG - Output : noden20.maas
2021-02-19 06:26:22,433 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:26:22,658 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 06:26:22,658 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 06:26:22,777 - DEBUG - Output : noden20.maas
2021-02-19 06:26:22,777 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:26:22,891 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 06:26:22,892 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 06:26:24,143 - DEBUG - Output : noden29
2021-02-19 06:26:24,144 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 06:26:24,248 - DEBUG - Output : noden29.maas
2021-02-19 06:26:24,249 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:26:24,471 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 06:26:24,471 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 06:26:24,578 - DEBUG - Output : 
2021-02-19 06:26:24,579 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:26:24,680 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 06:26:24,881 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 06:26:26,124 - DEBUG - Output : nodei34
2021-02-19 06:26:26,125 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 06:26:26,225 - DEBUG - Output : nodei34.maas
2021-02-19 06:26:26,225 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:26:26,442 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 06:26:26,443 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 06:26:26,544 - DEBUG - Output : nodei34.maas
2021-02-19 06:26:26,544 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:26:26,653 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 06:26:26,658 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 06:26:27,886 - DEBUG - Output : noden19
2021-02-19 06:26:27,887 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 06:26:27,996 - DEBUG - Output : noden19.maas
2021-02-19 06:26:27,997 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:26:28,165 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 06:26:28,166 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:26:28,264 - DEBUG - Output : 192.168.27.19/24
2021-02-19 06:26:28,264 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 06:26:28,367 - DEBUG - Output : noden19.maas
2021-02-19 06:26:28,367 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 06:26:29,088 - DEBUG - Output : nodec9
2021-02-19 06:26:29,088 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 06:26:29,171 - DEBUG - Output : nodec9.maas
2021-02-19 06:26:29,172 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:26:29,294 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 06:26:29,294 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:26:29,371 - DEBUG - Output : 192.168.27.9/24
2021-02-19 06:26:29,372 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 06:26:29,451 - DEBUG - Output : nodec9.maas
2021-02-19 06:26:29,451 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 06:26:29,831 - DEBUG - Output : noden18
2021-02-19 06:26:29,832 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 06:26:29,889 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 06:26:29,890 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:26:30,085 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 06:26:32,735 - DEBUG - Not creating keypair since it exists
2021-02-19 06:26:33,477 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:26:33,607 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:26:33,638 - INFO - Adding rules to the default security group in Project admin
2021-02-19 06:26:33,799 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:26:34,153 - INFO - ================================================================================
2021-02-19 06:26:34,154 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 06:26:34,155 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:26:35,454 - DEBUG - Skipping xmpp flap check
2021-02-19 06:26:35,455 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:26:35,455 - INFO - 
2021-02-19 06:26:39,921 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:26:56,682 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:27:39,662 - INFO - Unable to find contrail-kube-manager container on 192.168.7.29
2021-02-19 06:27:39,663 - INFO - Running docker restart contrailkubernetesmaster_kubemanager_1 -t 60 on noden29
2021-02-19 06:27:39,665 - DEBUG - [192.168.7.29]: Running cmd : docker restart contrailkubernetesmaster_kubemanager_1 -t 60
2021-02-19 06:27:41,618 - DEBUG - Output : contrailkubernetesmaster_kubemanager_1
2021-02-19 06:27:41,619 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:27:41,619 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:27:41,729 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:27:41,730 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 06:27:46,731 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:27:46,734 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:27:46,818 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:27:46,819 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 1
2021-02-19 06:27:51,820 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:27:51,821 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:27:51,916 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:27:51,916 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 2
2021-02-19 06:27:56,918 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:27:56,919 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:27:57,016 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:27:57,017 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 3
2021-02-19 06:28:02,017 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:28:02,019 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:28:02,102 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:28:02,103 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 4
2021-02-19 06:28:07,104 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:28:07,106 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:28:07,203 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:28:07,204 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 5
2021-02-19 06:28:12,205 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:28:12,207 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:28:12,293 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:28:12,293 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 6
2021-02-19 06:28:17,295 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:28:17,296 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:28:17,388 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:28:17,388 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 7
2021-02-19 06:28:22,390 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:28:22,391 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:28:22,484 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:28:22,484 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 8
2021-02-19 06:28:27,485 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:28:27,486 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:28:27,567 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:28:27,568 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 9
2021-02-19 06:28:27,568 - ERROR - Not all services up , Gave up!
2021-02-19 06:29:11,260 - ERROR - AssertionError
Python 3.6.8: /usr/bin/python3
Fri Feb 19 06:29:09 2021

A problem occurred in a Python script.  Here is the sequence of
function calls leading up to the error, in the order they occurred.

 /contrail-test/tcutils/wrappers.py in wrapper(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7ff0927ec748>, *args=(), **kwargs={})
   80             log.info('Initial checks done. Running the testcase now')
   81             log.info('')
   82             result = function(self, *args, **kwargs)
   83             if self.inputs.upgrade:
   84                 pid = os.getpid()
result = None
function = <function TestRestart.test_admin_project_with_kube_manager_restart>
self = <serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7ff0927ec748>
args = ()
kwargs = {}

 /contrail-test/serial_scripts/k8s_auth/test_restart.py in test_admin_project_with_kube_manager_restart(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7ff0927ec748>)
   45         self.inputs.restart_service('contrail-kube-manager', self.inputs.kube_manager_ips,
   46                                      container='contrailkubernetesmaster_kubemanager_1',
   47                                      verify_service=True)
   48         ResourceUtil.create_policy_and_perform_operations(
   49             resource_expectation=TestRestart.resource_expectation,
verify_service undefined

 /contrail-test/common/contrail_test_init.py in restart_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', host_ips=['192.168.7.29'], container='contrailkubernetesmaster_kubemanager_1', verify_service=True)
 1437                         container=None, verify_service=True):
 1438         self._action_on_service(service_name, 'restart', host_ips, container,
 1439             verify_service=verify_service)
 1440     # end restart_service
 1441 
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', event='restart', host_ips=['192.168.7.29'], container='contrailkubernetesmaster_kubemanager_1', verify_service=True)
 1417         if self.is_microservices_env and container:
 1418             return self._action_on_container(host_ips, event, container, services=services,
 1419                                              verify_service=verify_service)
 1420         _container = container
 1421         for service in services:
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_container(self=<common.contrail_test_init.ContrailTestInit object>, hosts=['192.168.7.29'], event='restart', container='contrailkubernetesmaster_kubemanager_1', services=['contrail-kube-manager'], verify_service=True, timeout='-t 60')
 1377                     else:
 1378                         service_status = self.verify_service_state(host_ip, container)[0]
 1379                     assert service_status
 1380     #end _action_on_container
 1381 
service_status = False
AssertionError: 
    __cause__ = None
    __class__ = <class 'AssertionError'>
    __context__ = None
    __delattr__ = <method-wrapper '__delattr__' of AssertionError object>
    __dict__ = {}
    __dir__ = <built-in method __dir__ of AssertionError object>
    __doc__ = 'Assertion failed.'
    __eq__ = <method-wrapper '__eq__' of AssertionError object>
    __format__ = <built-in method __format__ of AssertionError object>
    __ge__ = <method-wrapper '__ge__' of AssertionError object>
    __getattribute__ = <method-wrapper '__getattribute__' of AssertionError object>
    __gt__ = <method-wrapper '__gt__' of AssertionError object>
    __hash__ = <method-wrapper '__hash__' of AssertionError object>
    __init__ = <method-wrapper '__init__' of AssertionError object>
    __init_subclass__ = <built-in method __init_subclass__ of type object>
    __le__ = <method-wrapper '__le__' of AssertionError object>
    __lt__ = <method-wrapper '__lt__' of AssertionError object>
    __ne__ = <method-wrapper '__ne__' of AssertionError object>
    __new__ = <built-in method __new__ of type object>
    __reduce__ = <built-in method __reduce__ of AssertionError object>
    __reduce_ex__ = <built-in method __reduce_ex__ of AssertionError object>
    __repr__ = <method-wrapper '__repr__' of AssertionError object>
    __setattr__ = <method-wrapper '__setattr__' of AssertionError object>
    __setstate__ = <built-in method __setstate__ of AssertionError object>
    __sizeof__ = <built-in method __sizeof__ of AssertionError object>
    __str__ = <method-wrapper '__str__' of AssertionError object>
    __subclasshook__ = <built-in method __subclasshook__ of type object>
    __suppress_context__ = False
    __traceback__ = <traceback object>
    args = ()
    with_traceback = <built-in method with_traceback of AssertionError object>

The above is a description of an error in a Python program.  Here is
the original traceback:

Traceback (most recent call last):
  File "/contrail-test/tcutils/wrappers.py", line 82, in wrapper
    result = function(self, *args, **kwargs)
  File "/contrail-test/serial_scripts/k8s_auth/test_restart.py", line 47, in test_admin_project_with_kube_manager_restart
    verify_service=True)
  File "/contrail-test/common/contrail_test_init.py", line 1439, in restart_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1419, in _action_on_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1379, in _action_on_container
    assert service_status
AssertionError



2021-02-19 06:29:11,260 - DEBUG - Skipping xmpp flap check
2021-02-19 06:29:11,261 - INFO - 
2021-02-19 06:29:11,261 - INFO - END TEST : test_admin_project_with_kube_manager_restart : FAILED[0:02:37]
2021-02-19 06:29:11,261 - INFO - --------------------------------------------------------------------------------
2021-02-19 06:29:20,507 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 06:29:21,601 - DEBUG - Output : noden20
2021-02-19 06:29:21,601 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 06:29:21,722 - DEBUG - Output : noden20.maas
2021-02-19 06:29:21,722 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:29:21,930 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 06:29:21,931 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 06:29:22,044 - DEBUG - Output : noden20.maas
2021-02-19 06:29:22,045 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:29:22,177 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 06:29:22,178 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 06:29:23,243 - DEBUG - Output : noden29
2021-02-19 06:29:23,244 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 06:29:23,348 - DEBUG - Output : noden29.maas
2021-02-19 06:29:23,348 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:29:23,573 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 06:29:23,574 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 06:29:23,746 - DEBUG - Output : 
2021-02-19 06:29:23,746 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:29:23,855 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 06:29:24,049 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 06:29:25,357 - DEBUG - Output : nodei34
2021-02-19 06:29:25,357 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 06:29:25,469 - DEBUG - Output : nodei34.maas
2021-02-19 06:29:25,469 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:29:25,695 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 06:29:25,696 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 06:29:25,784 - DEBUG - Output : nodei34.maas
2021-02-19 06:29:25,784 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:29:25,882 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 06:29:25,883 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 06:29:26,896 - DEBUG - Output : noden19
2021-02-19 06:29:26,897 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 06:29:26,995 - DEBUG - Output : noden19.maas
2021-02-19 06:29:26,996 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:29:27,165 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 06:29:27,166 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:29:27,271 - DEBUG - Output : 192.168.27.19/24
2021-02-19 06:29:27,272 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 06:29:27,376 - DEBUG - Output : noden19.maas
2021-02-19 06:29:27,376 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 06:29:28,136 - DEBUG - Output : nodec9
2021-02-19 06:29:28,137 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 06:29:28,223 - DEBUG - Output : nodec9.maas
2021-02-19 06:29:28,224 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:29:28,336 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 06:29:28,336 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:29:28,419 - DEBUG - Output : 192.168.27.9/24
2021-02-19 06:29:28,420 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 06:29:28,503 - DEBUG - Output : nodec9.maas
2021-02-19 06:29:28,504 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 06:29:28,891 - DEBUG - Output : noden18
2021-02-19 06:29:28,892 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 06:29:28,948 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 06:29:28,949 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:29:29,144 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 06:29:30,572 - DEBUG - Not creating keypair since it exists
2021-02-19 06:29:31,171 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:29:31,295 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:29:31,325 - INFO - Adding rules to the default security group in Project admin
2021-02-19 06:29:31,454 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:29:31,808 - INFO - ================================================================================
2021-02-19 06:29:31,808 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 06:29:31,809 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:29:33,112 - DEBUG - Skipping xmpp flap check
2021-02-19 06:29:33,113 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:29:33,113 - INFO - 
2021-02-19 06:29:35,157 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:29:46,054 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:30:23,691 - INFO - Running docker restart contrailkubernetesmaster_kubemanager_1 -t 60 on noden29
2021-02-19 06:30:23,692 - DEBUG - [192.168.7.29]: Running cmd : docker restart contrailkubernetesmaster_kubemanager_1 -t 60
2021-02-19 06:30:25,375 - DEBUG - Output : contrailkubernetesmaster_kubemanager_1
2021-02-19 06:30:25,376 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:25,376 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:25,491 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:25,491 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 06:30:30,493 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:30,493 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:30,576 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:30,576 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 1
2021-02-19 06:30:35,577 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:35,578 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:35,657 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:35,657 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 2
2021-02-19 06:30:40,658 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:40,659 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:40,740 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:40,741 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 3
2021-02-19 06:30:45,741 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:45,742 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:45,824 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:45,824 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 4
2021-02-19 06:30:50,825 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:50,827 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:50,912 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:50,912 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 5
2021-02-19 06:30:55,913 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:30:55,914 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:30:55,995 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:30:55,995 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 6
2021-02-19 06:31:00,997 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:31:00,997 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:31:01,080 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:31:01,081 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 7
2021-02-19 06:31:06,081 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:31:06,082 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:31:06,166 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:31:06,166 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 8
2021-02-19 06:31:11,167 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:31:11,168 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:31:11,251 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:31:11,251 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 9
2021-02-19 06:31:11,252 - ERROR - Not all services up , Gave up!
2021-02-19 06:31:12,585 - ERROR - AssertionError
Python 3.6.8: /usr/bin/python3
Fri Feb 19 06:31:11 2021

A problem occurred in a Python script.  Here is the sequence of
function calls leading up to the error, in the order they occurred.

 /contrail-test/tcutils/wrappers.py in wrapper(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f96438ca748>, *args=(), **kwargs={})
   80             log.info('Initial checks done. Running the testcase now')
   81             log.info('')
   82             result = function(self, *args, **kwargs)
   83             if self.inputs.upgrade:
   84                 pid = os.getpid()
result = None
function = <function TestRestart.test_admin_project_with_kube_manager_restart>
self = <serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f96438ca748>
args = ()
kwargs = {}

 /contrail-test/serial_scripts/k8s_auth/test_restart.py in test_admin_project_with_kube_manager_restart(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7f96438ca748>)
   44         self.inputs.restart_service('contrail-kube-manager', self.inputs.kube_manager_ips,
   45                                      container='contrailkubernetesmaster_kubemanager_1',
   46                                      verify_service=True)
   47         time.sleep(30)
   48         ResourceUtil.create_policy_and_perform_operations(
verify_service undefined

 /contrail-test/common/contrail_test_init.py in restart_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', host_ips=['192.168.7.29'], container='contrailkubernetesmaster_kubemanager_1', verify_service=True)
 1437                         container=None, verify_service=True):
 1438         self._action_on_service(service_name, 'restart', host_ips, container,
 1439             verify_service=verify_service)
 1440     # end restart_service
 1441 
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', event='restart', host_ips=['192.168.7.29'], container='contrailkubernetesmaster_kubemanager_1', verify_service=True)
 1417         if self.is_microservices_env and container:
 1418             return self._action_on_container(host_ips, event, container, services=services,
 1419                                              verify_service=verify_service)
 1420         _container = container
 1421         for service in services:
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_container(self=<common.contrail_test_init.ContrailTestInit object>, hosts=['192.168.7.29'], event='restart', container='contrailkubernetesmaster_kubemanager_1', services=['contrail-kube-manager'], verify_service=True, timeout='-t 60')
 1377                     else:
 1378                         service_status = self.verify_service_state(host_ip, container)[0]
 1379                     assert service_status
 1380     #end _action_on_container
 1381 
service_status = False
AssertionError: 
    __cause__ = None
    __class__ = <class 'AssertionError'>
    __context__ = None
    __delattr__ = <method-wrapper '__delattr__' of AssertionError object>
    __dict__ = {}
    __dir__ = <built-in method __dir__ of AssertionError object>
    __doc__ = 'Assertion failed.'
    __eq__ = <method-wrapper '__eq__' of AssertionError object>
    __format__ = <built-in method __format__ of AssertionError object>
    __ge__ = <method-wrapper '__ge__' of AssertionError object>
    __getattribute__ = <method-wrapper '__getattribute__' of AssertionError object>
    __gt__ = <method-wrapper '__gt__' of AssertionError object>
    __hash__ = <method-wrapper '__hash__' of AssertionError object>
    __init__ = <method-wrapper '__init__' of AssertionError object>
    __init_subclass__ = <built-in method __init_subclass__ of type object>
    __le__ = <method-wrapper '__le__' of AssertionError object>
    __lt__ = <method-wrapper '__lt__' of AssertionError object>
    __ne__ = <method-wrapper '__ne__' of AssertionError object>
    __new__ = <built-in method __new__ of type object>
    __reduce__ = <built-in method __reduce__ of AssertionError object>
    __reduce_ex__ = <built-in method __reduce_ex__ of AssertionError object>
    __repr__ = <method-wrapper '__repr__' of AssertionError object>
    __setattr__ = <method-wrapper '__setattr__' of AssertionError object>
    __setstate__ = <built-in method __setstate__ of AssertionError object>
    __sizeof__ = <built-in method __sizeof__ of AssertionError object>
    __str__ = <method-wrapper '__str__' of AssertionError object>
    __subclasshook__ = <built-in method __subclasshook__ of type object>
    __suppress_context__ = False
    __traceback__ = <traceback object>
    args = ()
    with_traceback = <built-in method with_traceback of AssertionError object>

The above is a description of an error in a Python program.  Here is
the original traceback:

Traceback (most recent call last):
  File "/contrail-test/tcutils/wrappers.py", line 82, in wrapper
    result = function(self, *args, **kwargs)
  File "/contrail-test/serial_scripts/k8s_auth/test_restart.py", line 46, in test_admin_project_with_kube_manager_restart
    verify_service=True)
  File "/contrail-test/common/contrail_test_init.py", line 1439, in restart_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1419, in _action_on_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1379, in _action_on_container
    assert service_status
AssertionError



2021-02-19 06:31:12,586 - DEBUG - Skipping xmpp flap check
2021-02-19 06:31:12,586 - INFO - 
2021-02-19 06:31:12,586 - INFO - END TEST : test_admin_project_with_kube_manager_restart : FAILED[0:01:41]
2021-02-19 06:31:12,586 - INFO - --------------------------------------------------------------------------------
2021-02-19 06:36:16,020 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 06:36:17,109 - DEBUG - Output : noden20
2021-02-19 06:36:17,110 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 06:36:17,223 - DEBUG - Output : noden20.maas
2021-02-19 06:36:17,223 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:36:17,446 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 06:36:17,446 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 06:36:17,568 - DEBUG - Output : noden20.maas
2021-02-19 06:36:17,569 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:36:17,692 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 06:36:17,693 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 06:36:18,762 - DEBUG - Output : noden29
2021-02-19 06:36:18,763 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 06:36:18,867 - DEBUG - Output : noden29.maas
2021-02-19 06:36:18,867 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:36:19,090 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 06:36:19,091 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 06:36:19,199 - DEBUG - Output : noden29.maas
2021-02-19 06:36:19,199 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:36:19,300 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 06:36:19,503 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 06:36:20,816 - DEBUG - Output : nodei34
2021-02-19 06:36:20,816 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 06:36:20,926 - DEBUG - Output : nodei34.maas
2021-02-19 06:36:20,927 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:36:21,142 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 06:36:21,142 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 06:36:21,255 - DEBUG - Output : nodei34.maas
2021-02-19 06:36:21,256 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:36:21,372 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 06:36:21,373 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 06:36:22,407 - DEBUG - Output : noden19
2021-02-19 06:36:22,407 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 06:36:22,506 - DEBUG - Output : noden19.maas
2021-02-19 06:36:22,506 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:36:22,684 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 06:36:22,685 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:36:22,787 - DEBUG - Output : 192.168.27.19/24
2021-02-19 06:36:22,787 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 06:36:22,890 - DEBUG - Output : noden19.maas
2021-02-19 06:36:22,890 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 06:36:23,606 - DEBUG - Output : nodec9
2021-02-19 06:36:23,606 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 06:36:23,683 - DEBUG - Output : nodec9.maas
2021-02-19 06:36:23,684 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:36:23,807 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 06:36:23,807 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:36:23,891 - DEBUG - Output : 192.168.27.9/24
2021-02-19 06:36:23,892 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 06:36:23,971 - DEBUG - Output : nodec9.maas
2021-02-19 06:36:23,972 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 06:36:24,352 - DEBUG - Output : noden18
2021-02-19 06:36:24,353 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 06:36:24,410 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 06:36:24,410 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:36:24,603 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 06:36:26,510 - DEBUG - Not creating keypair since it exists
2021-02-19 06:36:27,224 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:36:27,350 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:36:27,380 - INFO - Adding rules to the default security group in Project admin
2021-02-19 06:36:27,530 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:36:27,882 - INFO - ================================================================================
2021-02-19 06:36:27,883 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 06:36:27,884 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:36:29,192 - DEBUG - Skipping xmpp flap check
2021-02-19 06:36:29,192 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:36:29,192 - INFO - 
2021-02-19 06:36:30,746 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:36:46,595 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:37:20,848 - INFO - Unable to find contrail-kube-manager container on 192.168.7.29
2021-02-19 06:37:20,848 - INFO - Running docker restart contrailkubernetesmaster_kubemanager_1 -t 60 on noden29
2021-02-19 06:37:20,849 - DEBUG - [192.168.7.29]: Running cmd : docker restart contrailkubernetesmaster_kubemanager_1 -t 60
2021-02-19 06:37:23,636 - DEBUG - Output : contrailkubernetesmaster_kubemanager_1
2021-02-19 06:37:23,636 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:23,637 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:23,752 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:23,752 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 0
2021-02-19 06:37:28,754 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:28,755 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:28,834 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:28,835 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 1
2021-02-19 06:37:33,835 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:33,836 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:33,916 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:33,917 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 2
2021-02-19 06:37:38,918 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:38,919 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:38,997 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:38,997 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 3
2021-02-19 06:37:43,999 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:44,001 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:44,086 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:44,087 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 4
2021-02-19 06:37:49,088 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:49,089 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:49,172 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:49,172 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 5
2021-02-19 06:37:54,173 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:54,173 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:54,253 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:54,253 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 6
2021-02-19 06:37:59,254 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:37:59,255 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:37:59,336 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:37:59,336 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 7
2021-02-19 06:38:04,337 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:38:04,337 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:38:04,418 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:38:04,419 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 8
2021-02-19 06:38:09,419 - DEBUG - Running command "systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'" on host "192.168.7.29" for service "contrailkubernetesmaster_kubemanager_1"
2021-02-19 06:38:09,420 - DEBUG - [192.168.7.29]: Running cmd : systemctl status  contrailkubernetesmaster_kubemanager_1 | grep Active| awk '{print $2}'
2021-02-19 06:38:09,499 - DEBUG - Output : Unit contrailkubernetesmaster_kubemanager_1.service could not be found.
2021-02-19 06:38:09,499 - DEBUG - Not all services up. Sleeping for 5 seconds. iteration: 9
2021-02-19 06:38:09,499 - ERROR - Not all services up , Gave up!
2021-02-19 06:38:10,832 - ERROR - AssertionError
Python 3.6.8: /usr/bin/python3
Fri Feb 19 06:38:09 2021

A problem occurred in a Python script.  Here is the sequence of
function calls leading up to the error, in the order they occurred.

 /contrail-test/tcutils/wrappers.py in wrapper(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7fb3439b4710>, *args=(), **kwargs={})
   80             log.info('Initial checks done. Running the testcase now')
   81             log.info('')
   82             result = function(self, *args, **kwargs)
   83             if self.inputs.upgrade:
   84                 pid = os.getpid()
result = None
function = <function TestRestart.test_admin_project_with_kube_manager_restart>
self = <serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7fb3439b4710>
args = ()
kwargs = {}

 /contrail-test/serial_scripts/k8s_auth/test_restart.py in test_admin_project_with_kube_manager_restart(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7fb3439b4710>)
   44         self.inputs.restart_service('contrail-kube-manager', self.inputs.kube_manager_ips,
   45                                      container='contrailkubernetesmaster_kubemanager_1',
   46                                      verify_service=True)
   47         time.sleep(30)
   48         ResourceUtil.create_policy_and_perform_operations(
verify_service undefined

 /contrail-test/common/contrail_test_init.py in restart_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', host_ips=['192.168.7.29'], container='contrailkubernetesmaster_kubemanager_1', verify_service=True)
 1437                         container=None, verify_service=True):
 1438         self._action_on_service(service_name, 'restart', host_ips, container,
 1439             verify_service=verify_service)
 1440     # end restart_service
 1441 
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_service(self=<common.contrail_test_init.ContrailTestInit object>, service_name='contrail-kube-manager', event='restart', host_ips=['192.168.7.29'], container='contrailkubernetesmaster_kubemanager_1', verify_service=True)
 1417         if self.is_microservices_env and container:
 1418             return self._action_on_container(host_ips, event, container, services=services,
 1419                                              verify_service=verify_service)
 1420         _container = container
 1421         for service in services:
verify_service = True

 /contrail-test/common/contrail_test_init.py in _action_on_container(self=<common.contrail_test_init.ContrailTestInit object>, hosts=['192.168.7.29'], event='restart', container='contrailkubernetesmaster_kubemanager_1', services=['contrail-kube-manager'], verify_service=True, timeout='-t 60')
 1377                     else:
 1378                         service_status = self.verify_service_state(host_ip, container)[0]
 1379                     assert service_status
 1380     #end _action_on_container
 1381 
service_status = False
AssertionError: 
    __cause__ = None
    __class__ = <class 'AssertionError'>
    __context__ = None
    __delattr__ = <method-wrapper '__delattr__' of AssertionError object>
    __dict__ = {}
    __dir__ = <built-in method __dir__ of AssertionError object>
    __doc__ = 'Assertion failed.'
    __eq__ = <method-wrapper '__eq__' of AssertionError object>
    __format__ = <built-in method __format__ of AssertionError object>
    __ge__ = <method-wrapper '__ge__' of AssertionError object>
    __getattribute__ = <method-wrapper '__getattribute__' of AssertionError object>
    __gt__ = <method-wrapper '__gt__' of AssertionError object>
    __hash__ = <method-wrapper '__hash__' of AssertionError object>
    __init__ = <method-wrapper '__init__' of AssertionError object>
    __init_subclass__ = <built-in method __init_subclass__ of type object>
    __le__ = <method-wrapper '__le__' of AssertionError object>
    __lt__ = <method-wrapper '__lt__' of AssertionError object>
    __ne__ = <method-wrapper '__ne__' of AssertionError object>
    __new__ = <built-in method __new__ of type object>
    __reduce__ = <built-in method __reduce__ of AssertionError object>
    __reduce_ex__ = <built-in method __reduce_ex__ of AssertionError object>
    __repr__ = <method-wrapper '__repr__' of AssertionError object>
    __setattr__ = <method-wrapper '__setattr__' of AssertionError object>
    __setstate__ = <built-in method __setstate__ of AssertionError object>
    __sizeof__ = <built-in method __sizeof__ of AssertionError object>
    __str__ = <method-wrapper '__str__' of AssertionError object>
    __subclasshook__ = <built-in method __subclasshook__ of type object>
    __suppress_context__ = False
    __traceback__ = <traceback object>
    args = ()
    with_traceback = <built-in method with_traceback of AssertionError object>

The above is a description of an error in a Python program.  Here is
the original traceback:

Traceback (most recent call last):
  File "/contrail-test/tcutils/wrappers.py", line 82, in wrapper
    result = function(self, *args, **kwargs)
  File "/contrail-test/serial_scripts/k8s_auth/test_restart.py", line 46, in test_admin_project_with_kube_manager_restart
    verify_service=True)
  File "/contrail-test/common/contrail_test_init.py", line 1439, in restart_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1419, in _action_on_service
    verify_service=verify_service)
  File "/contrail-test/common/contrail_test_init.py", line 1379, in _action_on_container
    assert service_status
AssertionError



2021-02-19 06:38:10,833 - DEBUG - Skipping xmpp flap check
2021-02-19 06:38:10,833 - INFO - 
2021-02-19 06:38:10,833 - INFO - END TEST : test_admin_project_with_kube_manager_restart : FAILED[0:01:43]
2021-02-19 06:38:10,833 - INFO - --------------------------------------------------------------------------------
2021-02-19 06:39:02,562 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 06:39:03,690 - DEBUG - Output : noden20
2021-02-19 06:39:03,690 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 06:39:03,805 - DEBUG - Output : noden20.maas
2021-02-19 06:39:03,805 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:39:04,028 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 06:39:04,029 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 06:39:04,144 - DEBUG - Output : noden20.maas
2021-02-19 06:39:04,145 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:39:04,274 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 06:39:04,275 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 06:39:05,335 - DEBUG - Output : noden29
2021-02-19 06:39:05,335 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 06:39:05,439 - DEBUG - Output : noden29.maas
2021-02-19 06:39:05,440 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:39:05,659 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 06:39:05,660 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 06:39:05,768 - DEBUG - Output : noden29.maas
2021-02-19 06:39:05,768 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:39:05,871 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 06:39:06,065 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 06:39:07,291 - DEBUG - Output : nodei34
2021-02-19 06:39:07,291 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 06:39:07,393 - DEBUG - Output : nodei34.maas
2021-02-19 06:39:07,394 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:39:07,609 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 06:39:07,610 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 06:39:07,718 - DEBUG - Output : nodei34.maas
2021-02-19 06:39:07,719 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:39:07,822 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 06:39:07,823 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 06:39:08,771 - DEBUG - Output : noden19
2021-02-19 06:39:08,771 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 06:39:08,873 - DEBUG - Output : noden19.maas
2021-02-19 06:39:08,874 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:39:09,049 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 06:39:09,050 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:39:09,152 - DEBUG - Output : 192.168.27.19/24
2021-02-19 06:39:09,152 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 06:39:09,254 - DEBUG - Output : noden19.maas
2021-02-19 06:39:09,254 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 06:39:09,974 - DEBUG - Output : nodec9
2021-02-19 06:39:09,974 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 06:39:10,056 - DEBUG - Output : nodec9.maas
2021-02-19 06:39:10,057 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:39:10,180 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 06:39:10,180 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:39:10,257 - DEBUG - Output : 192.168.27.9/24
2021-02-19 06:39:10,257 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 06:39:10,341 - DEBUG - Output : nodec9.maas
2021-02-19 06:39:10,341 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 06:39:10,723 - DEBUG - Output : noden18
2021-02-19 06:39:10,724 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 06:39:10,782 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 06:39:10,782 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:39:10,967 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 06:39:12,332 - DEBUG - Not creating keypair since it exists
2021-02-19 06:39:12,957 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:39:13,087 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:39:13,113 - INFO - Adding rules to the default security group in Project admin
2021-02-19 06:39:13,289 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:39:13,634 - INFO - ================================================================================
2021-02-19 06:39:13,634 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 06:39:13,635 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:39:14,960 - DEBUG - Skipping xmpp flap check
2021-02-19 06:39:14,960 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:39:14,960 - INFO - 
2021-02-19 06:39:16,388 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:39:32,253 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:40:10,004 - INFO - Running docker restart contrailkubernetesmaster_kubemanager_1 -t 60 on noden29
2021-02-19 06:40:10,005 - DEBUG - [192.168.7.29]: Running cmd : docker restart contrailkubernetesmaster_kubemanager_1 -t 60
2021-02-19 06:40:12,041 - DEBUG - Output : contrailkubernetesmaster_kubemanager_1
2021-02-19 06:40:12,041 - INFO - Unable to find contrail-kube-manager container on 192.168.7.29
2021-02-19 06:40:13,372 - ERROR - NameError
Python 3.6.8: /usr/bin/python3
Fri Feb 19 06:40:12 2021

A problem occurred in a Python script.  Here is the sequence of
function calls leading up to the error, in the order they occurred.

 /contrail-test/tcutils/wrappers.py in wrapper(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7fdac8be8780>, *args=(), **kwargs={})
   80             log.info('Initial checks done. Running the testcase now')
   81             log.info('')
   82             result = function(self, *args, **kwargs)
   83             if self.inputs.upgrade:
   84                 pid = os.getpid()
result = None
function = <function TestRestart.test_admin_project_with_kube_manager_restart>
self = <serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7fdac8be8780>
args = ()
kwargs = {}

 /contrail-test/serial_scripts/k8s_auth/test_restart.py in test_admin_project_with_kube_manager_restart(self=<serial_scripts.k8s_auth.test_restart.TestRestar...ith_kube_manager_restart[auth] id=0x7fdac8be8780>)
   45                                      container='contrailkubernetesmaster_kubemanager_1',
   46                                      verify_service=False)
   47         time.sleep(30)
   48         ResourceUtil.create_policy_and_perform_operations(
   49             resource_expectation=TestRestart.resource_expectation,
time undefined
NameError: name 'time' is not defined
    __cause__ = None
    __class__ = <class 'NameError'>
    __context__ = None
    __delattr__ = <method-wrapper '__delattr__' of NameError object>
    __dict__ = {}
    __dir__ = <built-in method __dir__ of NameError object>
    __doc__ = 'Name not found globally.'
    __eq__ = <method-wrapper '__eq__' of NameError object>
    __format__ = <built-in method __format__ of NameError object>
    __ge__ = <method-wrapper '__ge__' of NameError object>
    __getattribute__ = <method-wrapper '__getattribute__' of NameError object>
    __gt__ = <method-wrapper '__gt__' of NameError object>
    __hash__ = <method-wrapper '__hash__' of NameError object>
    __init__ = <method-wrapper '__init__' of NameError object>
    __init_subclass__ = <built-in method __init_subclass__ of type object>
    __le__ = <method-wrapper '__le__' of NameError object>
    __lt__ = <method-wrapper '__lt__' of NameError object>
    __ne__ = <method-wrapper '__ne__' of NameError object>
    __new__ = <built-in method __new__ of type object>
    __reduce__ = <built-in method __reduce__ of NameError object>
    __reduce_ex__ = <built-in method __reduce_ex__ of NameError object>
    __repr__ = <method-wrapper '__repr__' of NameError object>
    __setattr__ = <method-wrapper '__setattr__' of NameError object>
    __setstate__ = <built-in method __setstate__ of NameError object>
    __sizeof__ = <built-in method __sizeof__ of NameError object>
    __str__ = <method-wrapper '__str__' of NameError object>
    __subclasshook__ = <built-in method __subclasshook__ of type object>
    __suppress_context__ = False
    __traceback__ = <traceback object>
    args = ("name 'time' is not defined",)
    with_traceback = <built-in method with_traceback of NameError object>

The above is a description of an error in a Python program.  Here is
the original traceback:

Traceback (most recent call last):
  File "/contrail-test/tcutils/wrappers.py", line 82, in wrapper
    result = function(self, *args, **kwargs)
  File "/contrail-test/serial_scripts/k8s_auth/test_restart.py", line 47, in test_admin_project_with_kube_manager_restart
    time.sleep(30)
NameError: name 'time' is not defined



2021-02-19 06:40:13,372 - DEBUG - Skipping xmpp flap check
2021-02-19 06:40:13,373 - INFO - 
2021-02-19 06:40:13,373 - INFO - END TEST : test_admin_project_with_kube_manager_restart : FAILED[0:01:00]
2021-02-19 06:40:13,373 - INFO - --------------------------------------------------------------------------------
2021-02-19 06:40:38,547 - DEBUG - [192.168.7.20]: Running cmd : hostname
2021-02-19 06:40:39,630 - DEBUG - Output : noden20
2021-02-19 06:40:39,630 - DEBUG - [192.168.7.20]: Running cmd : hostname -f
2021-02-19 06:40:39,746 - DEBUG - Output : noden20.maas
2021-02-19 06:40:39,746 - DEBUG - [192.168.7.20]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:40:39,959 - DEBUG - Output : NAMES
webui_job_1
webui_web_1
analyticssnmp_snmp-collector_1
analyticssnmp_topology_1
analyticssnmp_nodemgr_1
analyticsalarm_kafka_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analytics_api_1
analytics_nodemgr_1
analytics_collector_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
redis_stunnel_1
redis_redis_1
control_named_1
control_nodemgr_1
control_dns_1
control_control_1
configdatabase_zookeeper_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_schema_1
configapi_svcmonitor_1
configapi_api_1
2021-02-19 06:40:39,959 - DEBUG - [192.168.7.20]: Running cmd : getent hosts 192.168.7.20 | head -n 1 | awk '{print $2}'
2021-02-19 06:40:40,084 - DEBUG - Output : noden20.maas
2021-02-19 06:40:40,085 - DEBUG - [192.168.7.20]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:40:40,196 - DEBUG - Output : 192.168.27.20/24
172.17.0.1/16
192.168.7.20/24
2021-02-19 06:40:40,197 - DEBUG - [192.168.7.29]: Running cmd : hostname
2021-02-19 06:40:41,255 - DEBUG - Output : noden29
2021-02-19 06:40:41,255 - DEBUG - [192.168.7.29]: Running cmd : hostname -f
2021-02-19 06:40:41,359 - DEBUG - Output : noden29.maas
2021-02-19 06:40:41,359 - DEBUG - [192.168.7.29]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:40:41,570 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
webui_job_1
webui_web_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticsdatabase_cassandra_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
contrailkubernetesmaster_kubemanager_1
redis_stunnel_1
redis_redis_1
control_named_1
control_control_1
control_nodemgr_1
control_dns_1
configdatabase_cassandra_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configdatabase_zookeeper_1
configapi_devicemgr_1
configapi_nodemgr_1
configapi_api_1
configapi_schema_1
configapi_svcmonitor_1
configapi_dnsmasq_1
2021-02-19 06:40:41,571 - DEBUG - [192.168.7.29]: Running cmd : getent hosts 192.168.7.29 | head -n 1 | awk '{print $2}'
2021-02-19 06:40:42,000 - DEBUG - Output : noden29.maas
2021-02-19 06:40:42,001 - DEBUG - [192.168.7.29]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:40:42,104 - DEBUG - Output : 192.168.7.29/24
172.17.0.1/16
192.168.27.29/24
2021-02-19 06:40:42,303 - DEBUG - [192.168.7.34]: Running cmd : hostname
2021-02-19 06:40:43,538 - DEBUG - Output : nodei34
2021-02-19 06:40:43,539 - DEBUG - [192.168.7.34]: Running cmd : hostname -f
2021-02-19 06:40:43,647 - DEBUG - Output : nodei34.maas
2021-02-19 06:40:43,648 - DEBUG - [192.168.7.34]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:40:43,872 - DEBUG - Output : NAMES
webui_web_1
webui_job_1
analyticsdatabase_cassandra_1
analyticsdatabase_query-engine_1
analyticsdatabase_nodemgr_1
analyticssnmp_snmp-collector_1
analyticssnmp_nodemgr_1
analyticssnmp_topology_1
analyticsalarm_nodemgr_1
analyticsalarm_alarm-gen_1
analyticsalarm_kafka_1
analytics_nodemgr_1
analytics_api_1
analytics_collector_1
redis_redis_1
redis_stunnel_1
control_named_1
control_dns_1
control_nodemgr_1
control_control_1
configdatabase_cassandra_1
configdatabase_zookeeper_1
configdatabase_rabbitmq_1
configdatabase_nodemgr_1
configapi_devicemgr_1
configapi_schema_1
configapi_dnsmasq_1
configapi_nodemgr_1
configapi_api_1
configapi_svcmonitor_1
2021-02-19 06:40:43,873 - DEBUG - [192.168.7.34]: Running cmd : getent hosts 192.168.7.34 | head -n 1 | awk '{print $2}'
2021-02-19 06:40:43,970 - DEBUG - Output : nodei34.maas
2021-02-19 06:40:43,971 - DEBUG - [192.168.7.34]: Running cmd : ip addr show | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:40:44,075 - DEBUG - Output : 192.168.7.34/24
192.168.27.34/24
172.17.0.1/16
2021-02-19 06:40:44,076 - DEBUG - [192.168.7.19]: Running cmd : hostname
2021-02-19 06:40:45,111 - DEBUG - Output : noden19
2021-02-19 06:40:45,111 - DEBUG - [192.168.7.19]: Running cmd : hostname -f
2021-02-19 06:40:45,216 - DEBUG - Output : noden19.maas
2021-02-19 06:40:45,216 - DEBUG - [192.168.7.19]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:40:45,377 - DEBUG - Output : NAMES
vrouter_nodemgr_1
vrouter_vrouter-agent_1
k8s_nginx_nginx_default_e122a4d5-13d3-4b5c-a917-c375f5a0cafa_0
k8s_cirros_cirros_default_17de7c97-01cd-4350-8401-3c2249528561_0
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_23
k8s_coredns_coredns-6b59b8bd9f-f7hgs_kube-system_efbcc658-8b0f-4982-860d-15887593226b_22
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-hcvxs_kube-system_64575096-0773-4881-8bba-f16559ca5fd6_0
k8s_k8s-keystone-auth_k8s-keystone-auth-bd454d699-4gj79_kube-system_83e45f58-f119-4828-9b80-a681ecebeb2b_0
2021-02-19 06:40:45,378 - DEBUG - [192.168.7.19]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:40:45,481 - DEBUG - Output : 192.168.27.19/24
2021-02-19 06:40:45,482 - DEBUG - [192.168.7.19]: Running cmd : getent hosts 192.168.27.19 | head -n 1 | awk '{print $2}'
2021-02-19 06:40:45,586 - DEBUG - Output : noden19.maas
2021-02-19 06:40:45,586 - DEBUG - [192.168.7.9]: Running cmd : hostname
2021-02-19 06:40:46,221 - DEBUG - Output : nodec9
2021-02-19 06:40:46,221 - DEBUG - [192.168.7.9]: Running cmd : hostname -f
2021-02-19 06:40:46,279 - DEBUG - Output : nodec9.maas
2021-02-19 06:40:46,280 - DEBUG - [192.168.7.9]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:40:46,375 - DEBUG - Output : NAMES
vrouter_vrouter-agent_1
vrouter_nodemgr_1
2021-02-19 06:40:46,375 - DEBUG - [192.168.7.9]: Running cmd : ip addr show dev vhost0 | grep 'inet .*/.* brd ' | awk '{print $2}'
2021-02-19 06:40:46,439 - DEBUG - Output : 192.168.27.9/24
2021-02-19 06:40:46,439 - DEBUG - [192.168.7.9]: Running cmd : getent hosts 192.168.27.9 | head -n 1 | awk '{print $2}'
2021-02-19 06:40:46,503 - DEBUG - Output : nodec9.maas
2021-02-19 06:40:46,504 - DEBUG - [192.168.7.18]: Running cmd : hostname
2021-02-19 06:40:46,884 - DEBUG - Output : noden18
2021-02-19 06:40:46,885 - DEBUG - [192.168.7.18]: Running cmd : hostname -f
2021-02-19 06:40:46,952 - DEBUG - Output : noden18.englab.juniper.net
2021-02-19 06:40:46,952 - DEBUG - [192.168.7.18]: Running cmd : docker ps -a 2>/dev/null | grep -v "/pause\|/usr/bin/pod\|nova_api_\|contrail.*init\|init.*contrail\|provisioner\|placement" | awk '{print $NF}'
2021-02-19 06:40:47,125 - DEBUG - Output : NAMES
nuthan_test8
nuthan_test2
nuthan_test7
tf_test
scale_test
nuthan_test
2021-02-19 06:40:48,667 - DEBUG - Not creating keypair since it exists
2021-02-19 06:40:49,430 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:40:49,553 - INFO - Using existing project ['admin_domain', 'admin'](6d52ffce-adf6-4a1b-b907-bb9dd95efc70)
2021-02-19 06:40:49,579 - INFO - Adding rules to the default security group in Project admin
2021-02-19 06:40:49,720 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:40:50,058 - INFO - ================================================================================
2021-02-19 06:40:50,058 - INFO - STARTING TEST    : test_admin_project_with_kube_manager_restart
2021-02-19 06:40:50,059 - INFO - TEST DESCRIPTION : 
        Description: Test to validate normal operations after kube manager restart
         Test steps:
                1. Create stackrc_dict for admin user
                2. Set the resource expectation list to all k8s resources
                3. Perform create and delete operations
                4. Restart kube manager
                5. Perform create and delete operations again
         Pass criteria: Even after kube manager restart, admin user must be able to perform all operations successfully
         Maintainer : nuthanc@juniper.net
        
2021-02-19 06:40:51,372 - DEBUG - Skipping xmpp flap check
2021-02-19 06:40:51,372 - INFO - Initial checks done. Running the testcase now
2021-02-19 06:40:51,373 - INFO - 
2021-02-19 06:40:52,826 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:41:02,659 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:41:37,065 - INFO - Unable to find contrail-kube-manager container on 192.168.7.29
2021-02-19 06:41:37,066 - INFO - Running docker restart contrailkubernetesmaster_kubemanager_1 -t 60 on noden29
2021-02-19 06:41:37,067 - DEBUG - [192.168.7.29]: Running cmd : docker restart contrailkubernetesmaster_kubemanager_1 -t 60
2021-02-19 06:41:38,824 - DEBUG - Output : contrailkubernetesmaster_kubemanager_1
2021-02-19 06:42:08,830 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:42:22,720 - DEBUG - Container None not in host 192.168.7.18, copying to  host itself
2021-02-19 06:42:57,184 - DEBUG - Skipping xmpp flap check
2021-02-19 06:42:57,184 - INFO - END TEST : test_admin_project_with_kube_manager_restart : PASSED[0:02:07]
2021-02-19 06:42:57,185 - INFO - --------------------------------------------------------------------------------
